{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, SimpleRNN, LSTM, GRU, Conv1D, Embedding, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import numpy as np\n",
    "import pronouncing\n",
    "import markovify\n",
    "import textstat\n",
    "import math\n",
    "\n",
    "import re\n",
    "import syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserspatricknaylorDesktopMetalDatalyricstxt\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'[^a-zA-Z]', '', lyric_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_path = '/Users/patricknaylor/Desktop/Metal/Data/lyrics_1.txt'\n",
    "\n",
    "with open(lyric_path, 'r') as file:\n",
    "    song = (file.read())\n",
    "    lyrics = song.replace('\\ufeff', '').split(\"\\n\")\n",
    "\n",
    "\n",
    "for line in lyrics:\n",
    "    line = re.sub(r'[^a-zA-Z]', '', line)\n",
    "\n",
    "#print(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For we've fallen on the wrath of god to he who discovers the key\n"
     ]
    }
   ],
   "source": [
    "markov_model = markovify.NewlineText(str(\"\\n\".join(lyrics)), well_formed=False, state_size=3)\n",
    "sentence = markov_model.make_sentence(tries=100)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = lyrics\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "\n",
    "V = len(tokenizer.word_index)+1\n",
    "seq = pad_sequences(tokenizer.texts_to_sequences(sequences), maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57446, 29) (57446, 20761)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = seq[:, :-1], tf.keras.utils.to_categorical(seq[:, -1], num_classes=V)\n",
    "\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 29)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 29, 512)           10629632  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 29, 512)           0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 150)               99450     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20761)             3134911   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13863993 (52.89 MB)\n",
      "Trainable params: 13863993 (52.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = 512\n",
    "\n",
    "#Simple RNN\n",
    "T = train_X.shape[1]\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V, D)(i)\n",
    "x = Dropout(0.2)(x)\n",
    "x = SimpleRNN(150)(x)\n",
    "x = Dense(V, activation=\"softmax\")(x)\n",
    "rnn_model = Model(i, x)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "rnn_model.compile(optimizer=adam, metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau , EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "es = EarlyStopping(monitor=\"loss\", mode=\"min\", verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1796/1796 [==============================] - 65s 36ms/step - loss: 7.7908 - accuracy: 0.0375 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1796/1796 [==============================] - 63s 35ms/step - loss: 6.3063 - accuracy: 0.0995 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1796/1796 [==============================] - 65s 36ms/step - loss: 5.1941 - accuracy: 0.1880 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 4.2384 - accuracy: 0.2911 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1796/1796 [==============================] - 66s 37ms/step - loss: 3.4573 - accuracy: 0.4022 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 2.8371 - accuracy: 0.4996 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 2.3712 - accuracy: 0.5712 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1796/1796 [==============================] - 275s 153ms/step - loss: 2.0250 - accuracy: 0.6278 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 1.7762 - accuracy: 0.6669 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1796/1796 [==============================] - 72s 40ms/step - loss: 1.5923 - accuracy: 0.6995 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1796/1796 [==============================] - 72s 40ms/step - loss: 1.4464 - accuracy: 0.7228 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 1.3338 - accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1796/1796 [==============================] - 74s 41ms/step - loss: 1.2508 - accuracy: 0.7554 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 1.1773 - accuracy: 0.7686 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.1250 - accuracy: 0.7767 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 1.0769 - accuracy: 0.7850 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 1.0423 - accuracy: 0.7899 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 1.0026 - accuracy: 0.7983 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.9795 - accuracy: 0.8013 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.9544 - accuracy: 0.8049 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.9296 - accuracy: 0.8104 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.9198 - accuracy: 0.8099 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.9063 - accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.8940 - accuracy: 0.8143 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.8833 - accuracy: 0.8170 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8726 - accuracy: 0.8181 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.8597 - accuracy: 0.8206 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.8596 - accuracy: 0.8204 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 0.8498 - accuracy: 0.8232 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 0.8480 - accuracy: 0.8219 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.8411 - accuracy: 0.8230 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.8439 - accuracy: 0.8206\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 0.8439 - accuracy: 0.8206 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.7009 - accuracy: 0.8502 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "1796/1796 [==============================] - 73s 40ms/step - loss: 0.6518 - accuracy: 0.8625 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.6355 - accuracy: 0.8660 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "1796/1796 [==============================] - 72s 40ms/step - loss: 0.6232 - accuracy: 0.8687 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "1796/1796 [==============================] - 72s 40ms/step - loss: 0.6162 - accuracy: 0.8702 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "1796/1796 [==============================] - 72s 40ms/step - loss: 0.6088 - accuracy: 0.8723 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 0.6027 - accuracy: 0.8733 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.6013 - accuracy: 0.8732 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "1796/1796 [==============================] - 321s 179ms/step - loss: 0.5968 - accuracy: 0.8742 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.5905 - accuracy: 0.8762 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5884 - accuracy: 0.8763 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.5891 - accuracy: 0.8755 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.5815 - accuracy: 0.8783 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.5783 - accuracy: 0.8787 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "1796/1796 [==============================] - 162s 90ms/step - loss: 0.5773 - accuracy: 0.8788 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "1796/1796 [==============================] - 766s 427ms/step - loss: 0.5772 - accuracy: 0.8788 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.5742 - accuracy: 0.8796 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.5709 - accuracy: 0.8808 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "1796/1796 [==============================] - 146s 81ms/step - loss: 0.5706 - accuracy: 0.8803 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5698 - accuracy: 0.8793 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.5655 - accuracy: 0.8805\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.5655 - accuracy: 0.8805 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.5156 - accuracy: 0.8902 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5042 - accuracy: 0.8921 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.5011 - accuracy: 0.8919 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4947 - accuracy: 0.8937 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4931 - accuracy: 0.8935 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4895 - accuracy: 0.8954 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4883 - accuracy: 0.8952 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4873 - accuracy: 0.8958 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.4864 - accuracy: 0.8951 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4859 - accuracy: 0.8956 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.4850 - accuracy: 0.8956\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1796/1796 [==============================] - 66s 37ms/step - loss: 0.4850 - accuracy: 0.8956 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4612 - accuracy: 0.9003 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "1796/1796 [==============================] - 74s 41ms/step - loss: 0.4575 - accuracy: 0.9004 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "1796/1796 [==============================] - 72s 40ms/step - loss: 0.4564 - accuracy: 0.9007 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.4529 - accuracy: 0.9017 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "1796/1796 [==============================] - 75s 42ms/step - loss: 0.4524 - accuracy: 0.9018 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.4523 - accuracy: 0.9016 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "1796/1796 [==============================] - 74s 41ms/step - loss: 0.4508 - accuracy: 0.9020 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4506 - accuracy: 0.9023 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.4515 - accuracy: 0.9012 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4492 - accuracy: 0.9023 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4485 - accuracy: 0.9023\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4485 - accuracy: 0.9023 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4362 - accuracy: 0.9052 - lr: 6.2500e-05\n",
      "Epoch 77/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4369 - accuracy: 0.9046 - lr: 6.2500e-05\n",
      "Epoch 78/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4355 - accuracy: 0.9051 - lr: 6.2500e-05\n",
      "Epoch 79/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.4340 - accuracy: 0.9049\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4339 - accuracy: 0.9050 - lr: 6.2500e-05\n",
      "Epoch 80/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4280 - accuracy: 0.9075 - lr: 3.1250e-05\n",
      "Epoch 81/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4280 - accuracy: 0.9067 - lr: 3.1250e-05\n",
      "Epoch 82/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4262 - accuracy: 0.9070 - lr: 3.1250e-05\n",
      "Epoch 83/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4257 - accuracy: 0.9067\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1796/1796 [==============================] - 73s 40ms/step - loss: 0.4257 - accuracy: 0.9067 - lr: 3.1250e-05\n",
      "Epoch 84/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4237 - accuracy: 0.9076 - lr: 1.5625e-05\n",
      "Epoch 85/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4224 - accuracy: 0.9084 - lr: 1.5625e-05\n",
      "Epoch 86/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4229 - accuracy: 0.9081 - lr: 1.5625e-05\n",
      "Epoch 87/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4225 - accuracy: 0.9082 - lr: 1.5625e-05\n",
      "Epoch 88/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.9084\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4230 - accuracy: 0.9084 - lr: 1.5625e-05\n",
      "Epoch 89/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4208 - accuracy: 0.9085 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4204 - accuracy: 0.9089 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4204 - accuracy: 0.9086 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1796/1796 [==============================] - 72s 40ms/step - loss: 0.4199 - accuracy: 0.9087 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4212 - accuracy: 0.9085 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4204 - accuracy: 0.9086 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4206 - accuracy: 0.9089 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4215 - accuracy: 0.9091 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1796/1796 [==============================] - 72s 40ms/step - loss: 0.4205 - accuracy: 0.9094 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4206 - accuracy: 0.9081 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4202 - accuracy: 0.9089 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1796/1796 [==============================] - 67s 38ms/step - loss: 0.4210 - accuracy: 0.9081 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "rnn_r = rnn_model.fit(train_X, train_y, epochs=100,callbacks=[learning_rate_reduction,es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_readability(input_bars):\n",
    "  avg_readability = 0\n",
    "  for bar in input_bars:\n",
    "    avg_readability += textstat.automated_readability_index(bar)\n",
    "  return avg_readability / len(input_bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rhyme_density(bars):\n",
    "  total_syllables = 0\n",
    "  rhymed_syllables = 0\n",
    "  for bar in bars:\n",
    "    for word in bar.split():\n",
    "      p = pronouncing.phones_for_word(word)\n",
    "      if len(p) == 0:\n",
    "        break\n",
    "      syllables = pronouncing.syllable_count(p[0])\n",
    "      total_syllables += syllables\n",
    "      has_rhyme = False\n",
    "      for rhyme in pronouncing.rhymes(word):\n",
    "        if has_rhyme:\n",
    "          break\n",
    "        for idx, r_bar in enumerate(bars):\n",
    "          if idx > 4:\n",
    "            break\n",
    "          if rhyme in r_bar:\n",
    "            rhymed_syllables += syllables\n",
    "            has_rhyme = True\n",
    "            break\n",
    "  return rhymed_syllables/total_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bar(seed_phrase, model, length_of_bar):\n",
    "  syl = 0\n",
    "  while syl <= length_of_bar:\n",
    "    seed_tokens = pad_sequences(tokenizer.texts_to_sequences([seed_phrase]), maxlen=29)\n",
    "    output_p = model.predict(seed_tokens)\n",
    "    output_word = np.argmax(output_p, axis=1)[0]-1\n",
    "    syl += syllables.estimate(str(list(tokenizer.word_index.items())[output_word][0]))\n",
    "    seed_phrase += \" \" + str(list(tokenizer.word_index.items())[output_word][0])\n",
    "  return seed_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bar(input_bar, artists_bars, artists_avg_readability, artists_avg_rhyme_idx):\n",
    "  gen_readability = textstat.automated_readability_index(input_bar)\n",
    "  gen_rhyme_idx = calc_rhyme_density(input_bar)\n",
    "  comp_bars = compare_bars(input_bar, artists_bars)\n",
    "\n",
    "  # Scores based off readability, rhyme index, and originality. The lower the score the better.\n",
    "  bar_score = (artists_avg_readability - gen_readability) + (artists_avg_rhyme_idx - gen_rhyme_idx) + comp_bars\n",
    "  return bar_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bars(input_bar, artists_bars):\n",
    "  '''\n",
    "    input_bars are the fire bars our AI generates\n",
    "    artists_bars are the original bars for the artist\n",
    "\n",
    "    The lower the score the better! We want unique bars\n",
    "  '''\n",
    "  # Converts sentences to matrix of token counts\n",
    "  avg_dist = 0\n",
    "  total_counted = 0\n",
    "  for bar in artists_bars:\n",
    "    v = CountVectorizer()\n",
    "    # Vectorize the sentences\n",
    "    word_vector = v.fit_transform([input_bar, bar])\n",
    "    # Compute the cosine distance between the sentence vectors\n",
    "    cos_dist = 1-pdist(word_vector.toarray(), 'cosine')[0]\n",
    "    if not math.isnan(cos_dist):\n",
    "      avg_dist += 1-pdist(word_vector.toarray(), 'cosine')[0]\n",
    "      total_counted += 1\n",
    "  return avg_dist/total_counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_song( model, intro_bar, artists_bars, length_of_bar, length_of_song=20, min_score_threshold=-0.2, max_score_threshold=0.2, tries=5):\n",
    "  artists_avg_readability = calc_readability(artists_bars)\n",
    "  artists_avg_rhyme_idx = calc_rhyme_density(artists_bars)\n",
    "  fire_song = [intro_bar]\n",
    "  bar_lengths = [7, 5]\n",
    "  cur_tries = 0\n",
    "  candidate_bars = []\n",
    "\n",
    "  while len(fire_song) < 3:\n",
    "    try:\n",
    "        seed_sentence = markov_model.make_sentence(tries=100).split(\" \")\n",
    "        print('Seed Sentence: ', seed_sentence)\n",
    "        seed_sentence = \" \".join(fire_song[-1].split(' ')[-3:]) + \" \".join(seed_sentence[:2])\n",
    "    except:\n",
    "        pass\n",
    "    cur_tries += 1\n",
    "    bar = generate_bar(seed_sentence, model, rand.randrange(4, bar_lengths[len(fire_song)-1]))\n",
    "    #print(bar)\n",
    "    bar_score = score_bar(bar, lyrics, artists_avg_readability, artists_avg_rhyme_idx) \n",
    "    candidate_bars.append((bar_score, bar))\n",
    "\n",
    "\n",
    "    if bar_score <= max_score_threshold and bar_score >= min_score_threshold and syllables.estimate(' '.join(bar.split(' ')[3:]) == bar_lengths[len(fire_song) - 1]):\n",
    "      fire_song.append(' '.join(bar.split(' ')[3:]))\n",
    "      cur_tries = 0\n",
    "      print(\"Generated Bar: \", len(fire_song))\n",
    "\n",
    "    if cur_tries >= tries:\n",
    "      lowest_score = np.Infinity\n",
    "      best_bar = \"\"\n",
    "      for bar in candidate_bars:\n",
    "        if bar[0] < lowest_score & syllables.estimate(' '.join(bar[1].split(' ')[3:])) == bar_lengths[len(fire_song) - 1]:\n",
    "          best_bar = bar[1]\n",
    "          candidate_bars = []\n",
    "      \n",
    "      fire_song.append(' '.join(best_bar.split(' ')[3:]))\n",
    "      print(\"Generated Bar: \", len(fire_song))\n",
    "      cur_tries = 0\n",
    "      \n",
    "  print(\"Generated song with avg rhyme density: \", calc_rhyme_density(fire_song), \"and avg readability of: \", calc_readability(fire_song))\n",
    "  return fire_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Seed Sentence:  ['Forged', 'into', 'steel,', 'with', 'the', 'edge', 'of', 'the', 'abyss', 'open', 'and', 'swallow', 'our', 'enemies', 'whole']\n",
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m rnn \u001b[39m=\u001b[39m generate_song( rnn_model, \u001b[39m'\u001b[39;49m\u001b[39mDeath is consuming the earth\u001b[39;49m\u001b[39m'\u001b[39;49m, lyrics, length_of_bar \u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m , tries\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSong Generated with SimpleRNN:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m rnn:\n",
      "Cell \u001b[0;32mIn[48], line 17\u001b[0m, in \u001b[0;36mgenerate_song\u001b[0;34m(model, intro_bar, artists_bars, length_of_bar, length_of_song, min_score_threshold, max_score_threshold, tries)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     16\u001b[0m cur_tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 17\u001b[0m bar \u001b[39m=\u001b[39m generate_bar(seed_sentence, model, rand\u001b[39m.\u001b[39;49mrandrange(\u001b[39m4\u001b[39;49m, bar_lengths[\u001b[39mlen\u001b[39;49m(fire_song)\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]))\n\u001b[1;32m     18\u001b[0m \u001b[39m#print(bar)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m bar_score \u001b[39m=\u001b[39m score_bar(bar, lyrics, artists_avg_readability, artists_avg_rhyme_idx) \n",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m, in \u001b[0;36mgenerate_bar\u001b[0;34m(seed_phrase, model, length_of_bar)\u001b[0m\n\u001b[1;32m      5\u001b[0m   output_p \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(seed_tokens)\n\u001b[1;32m      6\u001b[0m   output_word \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(output_p, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m   syl \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m syllables\u001b[39m.\u001b[39;49mestimate(output_word)\n\u001b[1;32m      8\u001b[0m   seed_phrase \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mlist\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index\u001b[39m.\u001b[39mitems())[output_word][\u001b[39m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m seed_phrase\n",
      "File \u001b[0;32m~/miniconda3/envs/RE/lib/python3.11/site-packages/syllables/__init__.py:201\u001b[0m, in \u001b[0;36mestimate\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mestimate\u001b[39m(word):\n\u001b[1;32m    188\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Estimates the number of syllables in an English-langauge word\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[39m    Parameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     parts \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msplit(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[^aeiouy]+\u001b[39m\u001b[39m\"\u001b[39m, word\u001b[39m.\u001b[39;49mlower())\n\u001b[1;32m    202\u001b[0m     valid_parts \u001b[39m=\u001b[39m []\n\u001b[1;32m    204\u001b[0m     \u001b[39mfor\u001b[39;00m part \u001b[39min\u001b[39;00m parts:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "rnn = generate_song( rnn_model, 'Death is consuming the earth', lyrics, length_of_bar =12 , tries=25)\n",
    "\n",
    "print(\"Song Generated with SimpleRNN:\")\n",
    "for line in rnn:\n",
    "  print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "#Clean code\n",
    "#comment\n",
    "#reformat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('RE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36173ce3d0ba65c4d5858397b29ed022201e85e6fa210e17017ccab12f085240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
