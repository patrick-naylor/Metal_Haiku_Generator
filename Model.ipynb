{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metal Haiku Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, LSTM, GRU, Conv1D, Embedding, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import numpy as np\n",
    "import pronouncing\n",
    "import markovify\n",
    "import textstat\n",
    "import math\n",
    "\n",
    "import re\n",
    "import syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load lyric dataset scraped from scraping.ipynb\n",
    "##### Remove any non letter elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_path = '/Users/patricknaylor/Desktop/Metal/Data/lyrics_1.txt'\n",
    "\n",
    "with open(lyric_path, 'r') as file:\n",
    "    song = (file.read())\n",
    "    lyrics = song.replace('\\ufeff', '').split(\"\\n\")\n",
    "\n",
    "\n",
    "for line in lyrics:\n",
    "    line = re.sub(r'[^a-zA-Z]', '', line)\n",
    "    line = re.sub(r'x2', '', line)\n",
    "\n",
    "#print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create markov model to generate seed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model = markovify.NewlineText(str(\"\\n\".join(lyrics)), well_formed=False, state_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize lyric database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = lyrics\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "\n",
    "V = len(tokenizer.word_index)+1\n",
    "seq = pad_sequences(tokenizer.texts_to_sequences(sequences), maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57446, 29) (57446, 20761)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = seq[:, :-1], tf.keras.utils.to_categorical(seq[:, -1], num_classes=V)\n",
    "\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define rnn model to generate song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 29)]              0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 29, 512)           10629632  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 29, 512)           0         \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 150)               99450     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20761)             3134911   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13863993 (52.89 MB)\n",
      "Trainable params: 13863993 (52.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = 512\n",
    "\n",
    "#Simple RNN\n",
    "T = train_X.shape[1]\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V, D)(i)\n",
    "x = Dropout(0.2)(x)\n",
    "x = SimpleRNN(150)(x)\n",
    "x = Dense(V, activation=\"softmax\")(x)\n",
    "rnn_model = Model(i, x)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "rnn_model.compile(optimizer=adam, metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau , EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "es = EarlyStopping(monitor=\"loss\", mode=\"min\", verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1796/1796 [==============================] - 77s 43ms/step - loss: 7.8723 - accuracy: 0.0312 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 6.3590 - accuracy: 0.0967 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 5.1645 - accuracy: 0.1927 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 4.1408 - accuracy: 0.3083 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 3.3173 - accuracy: 0.4255 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 2.6795 - accuracy: 0.5253 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1796/1796 [==============================] - 77s 43ms/step - loss: 2.2064 - accuracy: 0.6011 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1796/1796 [==============================] - 75s 42ms/step - loss: 1.8788 - accuracy: 0.6526 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 1.6466 - accuracy: 0.6891 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.4738 - accuracy: 0.7202 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.3444 - accuracy: 0.7412 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.2430 - accuracy: 0.7586 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.1660 - accuracy: 0.7720 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 1.1042 - accuracy: 0.7826 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 1.0555 - accuracy: 0.7911 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 1.0136 - accuracy: 0.7968 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.9797 - accuracy: 0.8038 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.9486 - accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.9283 - accuracy: 0.8127 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.9123 - accuracy: 0.8148 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8914 - accuracy: 0.8197 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8821 - accuracy: 0.8184 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8661 - accuracy: 0.8225 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8569 - accuracy: 0.8234 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.8478 - accuracy: 0.8248 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8434 - accuracy: 0.8248 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8336 - accuracy: 0.8262 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8308 - accuracy: 0.8264 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8314 - accuracy: 0.8264 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8174 - accuracy: 0.8288 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8200 - accuracy: 0.8269 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.8217 - accuracy: 0.8261 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.8195 - accuracy: 0.8261\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.8195 - accuracy: 0.8261 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.6719 - accuracy: 0.8567 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.6279 - accuracy: 0.8675 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.6137 - accuracy: 0.8689 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "1796/1796 [==============================] - 77s 43ms/step - loss: 0.6077 - accuracy: 0.8713 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 0.6003 - accuracy: 0.8730 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 0.5926 - accuracy: 0.8756 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5874 - accuracy: 0.8769 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "1796/1796 [==============================] - 75s 42ms/step - loss: 0.5815 - accuracy: 0.8774 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5789 - accuracy: 0.8777 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5774 - accuracy: 0.8784 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5724 - accuracy: 0.8801 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.5708 - accuracy: 0.8796 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5706 - accuracy: 0.8811 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5640 - accuracy: 0.8823 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5660 - accuracy: 0.8805 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.5609 - accuracy: 0.8825 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.5597 - accuracy: 0.8827 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5633 - accuracy: 0.8806 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5603 - accuracy: 0.8813 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.5550 - accuracy: 0.8823\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5551 - accuracy: 0.8822 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.5081 - accuracy: 0.8915 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4948 - accuracy: 0.8941 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4921 - accuracy: 0.8942 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4888 - accuracy: 0.8947 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "1796/1796 [==============================] - 74s 41ms/step - loss: 0.4840 - accuracy: 0.8966 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4826 - accuracy: 0.8973 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4815 - accuracy: 0.8959 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4800 - accuracy: 0.8972 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.8953\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1796/1796 [==============================] - 66s 37ms/step - loss: 0.4807 - accuracy: 0.8953 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "1796/1796 [==============================] - 66s 37ms/step - loss: 0.4575 - accuracy: 0.9004 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4522 - accuracy: 0.9015 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4504 - accuracy: 0.9022 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4494 - accuracy: 0.9013 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4479 - accuracy: 0.9027 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4475 - accuracy: 0.9024 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.4481 - accuracy: 0.9019 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.4464 - accuracy: 0.9028\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4463 - accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4324 - accuracy: 0.9057 - lr: 6.2500e-05\n",
      "Epoch 72/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4339 - accuracy: 0.9063 - lr: 6.2500e-05\n",
      "Epoch 73/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4335 - accuracy: 0.9057 - lr: 6.2500e-05\n",
      "Epoch 74/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4329 - accuracy: 0.9050 - lr: 6.2500e-05\n",
      "Epoch 75/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.9056\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4316 - accuracy: 0.9056 - lr: 6.2500e-05\n",
      "Epoch 76/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4259 - accuracy: 0.9069 - lr: 3.1250e-05\n",
      "Epoch 77/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4238 - accuracy: 0.9077 - lr: 3.1250e-05\n",
      "Epoch 78/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4241 - accuracy: 0.9078 - lr: 3.1250e-05\n",
      "Epoch 79/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4226 - accuracy: 0.9077 - lr: 3.1250e-05\n",
      "Epoch 80/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.9077\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4229 - accuracy: 0.9077 - lr: 3.1250e-05\n",
      "Epoch 81/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4200 - accuracy: 0.9086 - lr: 1.5625e-05\n",
      "Epoch 82/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4204 - accuracy: 0.9083 - lr: 1.5625e-05\n",
      "Epoch 83/100\n",
      "1796/1796 [==============================] - 67s 38ms/step - loss: 0.4214 - accuracy: 0.9076 - lr: 1.5625e-05\n",
      "Epoch 84/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4196 - accuracy: 0.9088 - lr: 1.5625e-05\n",
      "Epoch 85/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4177 - accuracy: 0.9096 - lr: 1.5625e-05\n",
      "Epoch 86/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4189 - accuracy: 0.9089 - lr: 1.5625e-05\n",
      "Epoch 87/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4195 - accuracy: 0.9087 - lr: 1.5625e-05\n",
      "Epoch 88/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.9088\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4191 - accuracy: 0.9088 - lr: 1.5625e-05\n",
      "Epoch 89/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4175 - accuracy: 0.9088 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4175 - accuracy: 0.9094 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4179 - accuracy: 0.9085 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4169 - accuracy: 0.9095 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.4173 - accuracy: 0.9094 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4162 - accuracy: 0.9093 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4179 - accuracy: 0.9082 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4165 - accuracy: 0.9096 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4175 - accuracy: 0.9087 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4170 - accuracy: 0.9087 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4162 - accuracy: 0.9090 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4182 - accuracy: 0.9079 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "rnn_r = rnn_model.fit(train_X, train_y, epochs=100,callbacks=[learning_rate_reduction,es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring metrics for lyrical output based on readabilty and rhyme frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_readability(input_lines):\n",
    "  avg_readability = 0\n",
    "  for line in input_lines:\n",
    "    avg_readability += textstat.automated_readability_index(line)\n",
    "  return avg_readability / len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rhyme_density(lines):\n",
    "  total_syllables = 0\n",
    "  rhymed_syllables = 0\n",
    "  for line in lines:\n",
    "    for word in line.split():\n",
    "      p = pronouncing.phones_for_word(word)\n",
    "      if len(p) == 0:\n",
    "        break\n",
    "      syllables = pronouncing.syllable_count(p[0])\n",
    "      total_syllables += syllables\n",
    "      has_rhyme = False\n",
    "      for rhyme in pronouncing.rhymes(word):\n",
    "        if has_rhyme:\n",
    "          break\n",
    "        for idx, r_line in enumerate(lines):\n",
    "          if idx > 4:\n",
    "            break\n",
    "          if rhyme in r_line:\n",
    "            rhymed_syllables += syllables\n",
    "            has_rhyme = True\n",
    "            break\n",
    "  return rhymed_syllables/total_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_line(input_line, artists_lines, artists_avg_readability, artists_avg_rhyme_idx):\n",
    "  gen_readability = textstat.automated_readability_index(input_line)\n",
    "  gen_rhyme_idx = calc_rhyme_density(input_line)\n",
    "  comp_lines = compare_lines(input_line, artists_lines)\n",
    "\n",
    "  # Scores based off readability, rhyme index, and originality. The lower the score the better.\n",
    "  line_score = (artists_avg_readability - gen_readability) + (artists_avg_rhyme_idx - gen_rhyme_idx) + comp_lines\n",
    "  return line_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lines(input_line, artists_lines):\n",
    "  '''\n",
    "    input_lines are the fire lines our AI generates\n",
    "    artists_lines are the original lines for the artist\n",
    "\n",
    "    The lower the score the better! We want unique lines\n",
    "  '''\n",
    "  # Converts sentences to matrix of token counts\n",
    "  avg_dist = 0\n",
    "  total_counted = 0\n",
    "  for line in artists_lines:\n",
    "    v = CountVectorizer()\n",
    "    # Vectorize the sentences\n",
    "    word_vector = v.fit_transform([input_line, line])\n",
    "    # Compute the cosine distance between the sentence vectors\n",
    "    cos_dist = 1-pdist(word_vector.toarray(), 'cosine')[0]\n",
    "    if not math.isnan(cos_dist):\n",
    "      avg_dist += 1-pdist(word_vector.toarray(), 'cosine')[0]\n",
    "      total_counted += 1\n",
    "  return avg_dist/total_counted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model\n",
    "Model is run based on seed sentence. Seed sentence combines final words from previous line and markov generated sentence. Generates lines with syllable count close to required by a haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_line(seed_phrase, model, length_of_line):\n",
    "  seed_words = ' '.join(seed_phrase.split(' ')[-2:])\n",
    "  syl = 0 + syllables.estimate(seed_words)\n",
    "  while syl < length_of_line:\n",
    "    seed_tokens = pad_sequences(tokenizer.texts_to_sequences([seed_phrase]), maxlen=29)\n",
    "    output_p = model.predict(seed_tokens)\n",
    "    output_word = np.argmax(output_p, axis=1)[0]-1\n",
    "    syl += syllables.estimate(str(list(tokenizer.word_index.items())[output_word][0]))\n",
    "    seed_phrase += \" \" + str(list(tokenizer.word_index.items())[output_word][0])\n",
    "  return seed_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Haiku\n",
    "Generate haiku by using a user defined first line. Generate the next two lines by creating a defined number of attempts and selecting the highest scoring option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_haiku( model, intro_line, artists_lines, length_of_line, length_of_song=20, min_score_threshold=-0.2, max_score_threshold=0.2, tries=5):\n",
    "  artists_avg_readability = calc_readability(artists_lines)\n",
    "  artists_avg_rhyme_idx = calc_rhyme_density(artists_lines)\n",
    "  fire_song = [intro_line + \" \"]\n",
    "  line_lengths = [7, 5]\n",
    "  cur_tries = 0\n",
    "  candidate_lines = []\n",
    "\n",
    "  while len(fire_song) < 3:\n",
    "    try:\n",
    "        seed_sentence = markov_model.make_sentence(tries=100).split(\" \")\n",
    "        print('Seed Sentence: ', seed_sentence)\n",
    "        seed_sentence = \" \".join(fire_song[-1].split(' ')[-3:]) + \" \".join(seed_sentence[:2])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    line = generate_line(seed_sentence, model, line_lengths[len(fire_song)-1])\n",
    "    print(syllables.estimate(' '.join(line.split(' ')[2:])))\n",
    "    if (syllables.estimate(' '.join(line.split(' ')[2:])) == line_lengths[len(fire_song) - 1]):\n",
    "      cur_tries += 1\n",
    "      print(cur_tries)\n",
    "    #print(line)\n",
    "    line_score = score_line(line, lyrics, artists_avg_readability, artists_avg_rhyme_idx) \n",
    "    candidate_lines.append((line_score, line))\n",
    "\n",
    "\n",
    "    if line_score <= max_score_threshold and line_score >= min_score_threshold and (syllables.estimate(' '.join(line.split(' ')[3:])) == line_lengths[len(fire_song) - 1]):\n",
    "      fire_song.append(' '.join(line.split(' ')[2:]) + \" \")\n",
    "      cur_tries = 0\n",
    "      print(\"Generated line: \", len(fire_song))\n",
    "\n",
    "    if cur_tries >= tries:\n",
    "      lowest_score = np.Infinity\n",
    "      best_line = \"\"\n",
    "      for line in candidate_lines:\n",
    "        if (line[0] < lowest_score) and (syllables.estimate(' '.join(line[1].split(' ')[2:])) == line_lengths[len(fire_song) - 1]):\n",
    "          best_line = line[1]\n",
    "          candidate_lines = []\n",
    "      \n",
    "      fire_song.append(' '.join(best_line.split(' ')[2:]) + \" \")\n",
    "      print(\"Generated line: \", len(fire_song))\n",
    "      cur_tries = 0\n",
    "      \n",
    "  print(\"Generated song with avg rhyme density: \", calc_rhyme_density(fire_song), \"and avg readability of: \", calc_readability(fire_song))\n",
    "  return fire_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Seed Sentence:  ['The', 'cure', 'is', 'right', 'in', 'my', 'mind.From', 'the', 'time', 'of', 'Satan', 'will', 'thrive!When', 'you', 'were', 'alive', 'and', 'when', 'you', 'come', 'to', 'see,', 'that', 'it', 'is', 'all', 'a', 'joke,']\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "8\n",
      "Seed Sentence:  ['I', 'fly', 'through', 'the', 'night']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "7\n",
      "1\n",
      "Seed Sentence:  ['Trust', 'is', 'dead', 'and', 'the', 'dying']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "8\n",
      "Seed Sentence:  ['By', 'the', 'grace', 'of', 'rationality']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "8\n",
      "Generated line:  2\n",
      "Seed Sentence:  ['Lures', 'all', 'to', 'the', 'poor']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "6\n",
      "Seed Sentence:  ['When', 'the', 'mind', 'and', 'soul']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "5\n",
      "1\n",
      "Seed Sentence:  [\"She's\", 'got', 'the', 'eyes', 'of', 'the', 'world.']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "5\n",
      "2\n",
      "Seed Sentence:  ['Off', 'in', 'my', 'own', 'hands,']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "5\n",
      "3\n",
      "Seed Sentence:  ['Never', 'draining,', 'always', 'running,', 'oh', 'to', 'have', 'the', 'agony', 'bleed', 'from', 'my', 'eyes', 'Find', 'your', 'simplified', 'peace', 'in', 'the', 'hole']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "5\n",
      "4\n",
      "Seed Sentence:  ['I', 'am', 'the', 'only', 'one', 'to', 'see.']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "5\n",
      "5\n",
      "Generated line:  3\n",
      "Generated song with avg rhyme density:  0.47058823529411764 and avg readability of:  -0.39999999999999974\n",
      "Song Generated with SimpleRNN:\n",
      "Death is consuming \n",
      "By the past knows game better \n",
      "I am real shot me \n"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "rnn = generate_haiku( rnn_model, 'Death is consuming', lyrics, length_of_line =12 , tries=5)\n",
    "\n",
    "print(\"Song Generated with SimpleRNN:\")\n",
    "for line in rnn:\n",
    "  print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('RE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36173ce3d0ba65c4d5858397b29ed022201e85e6fa210e17017ccab12f085240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
