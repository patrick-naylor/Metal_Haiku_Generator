{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metal Haiku Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "The primary objective of this notebook is to train a neural network on a dataset of metal lyrics and then leverage the trained model to generate haikus that encapsulate the intense and emotive essence of metal music within the concise form of a haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, LSTM, GRU, Conv1D, Embedding, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import numpy as np\n",
    "import pronouncing\n",
    "import markovify\n",
    "import textstat\n",
    "import math\n",
    "\n",
    "import re\n",
    "import syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load lyric dataset scraped from scraping.ipynb\n",
    "The cornerstone of this project is the dataset of metal lyrics, procured from the authoritative repository of metal music, metal-archives.com. This section elucidates the approach taken to acquire and preprocess the dataset.\n",
    "\n",
    "Scraping Process:\n",
    "The scraping process was executed using the Beautiful Soup Python library within the Jupyter Notebook titled \"scraping.ipynb.\" This notebook interacted with the metal-archives.com website, extracting lyrics exclusively from bands based in the United States. This selection criterion aimed to primarily retrieve lyrics written in English, ensuring linguistic consistency for the subsequent text generation phase.\n",
    "\n",
    "Tokenization and Preprocessing:\n",
    "Post-scraping, the dataset underwent meticulous preprocessing to ensure optimum quality and uniformity. The preprocessing steps included:\n",
    "\n",
    "Tokenization: The scraped lyrics were tokenized into individual words. This process transformed the continuous text into a sequence of discrete tokens, allowing the neural network to comprehend and learn the underlying language patterns within the lyrics.\n",
    "\n",
    "Removal of Non-Alphabetical Elements: Non-alphabetical elements such as special characters and symbols unrelated to language were systematically removed. This cleaning step aimed to refine the text corpus and eliminate extraneous noise that could hinder subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric_path = '/Users/patricknaylor/Desktop/Metal/Data/lyrics_1.txt'\n",
    "\n",
    "with open(lyric_path, 'r') as file:\n",
    "    song = (file.read())\n",
    "    lyrics = song.replace('\\ufeff', '').split(\"\\n\")\n",
    "\n",
    "\n",
    "for line in lyrics:\n",
    "    line = re.sub(r'[^a-zA-Z]', '', line)\n",
    "    line = re.sub(r'x2', '', line)\n",
    "\n",
    "#print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create markov model to generate seed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model = markovify.NewlineText(str(\"\\n\".join(lyrics)), well_formed=False, state_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize lyric database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = lyrics\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "\n",
    "V = len(tokenizer.word_index)+1\n",
    "seq = pad_sequences(tokenizer.texts_to_sequences(sequences), maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57446, 29) (57446, 20761)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = seq[:, :-1], tf.keras.utils.to_categorical(seq[:, -1], num_classes=V)\n",
    "\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define rnn model to generate song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 29)]              0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 29, 512)           10629632  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 29, 512)           0         \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 150)               99450     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20761)             3134911   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13863993 (52.89 MB)\n",
      "Trainable params: 13863993 (52.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "D = 512\n",
    "\n",
    "#Simple RNN\n",
    "T = train_X.shape[1]\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(V, D)(i)\n",
    "x = Dropout(0.2)(x)\n",
    "x = SimpleRNN(150)(x)\n",
    "x = Dense(V, activation=\"softmax\")(x)\n",
    "rnn_model = Model(i, x)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "rnn_model.compile(optimizer=adam, metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau , EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='accuracy', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "es = EarlyStopping(monitor=\"loss\", mode=\"min\", verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1796/1796 [==============================] - 77s 43ms/step - loss: 7.8723 - accuracy: 0.0312 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 6.3590 - accuracy: 0.0967 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 5.1645 - accuracy: 0.1927 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 4.1408 - accuracy: 0.3083 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 3.3173 - accuracy: 0.4255 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 2.6795 - accuracy: 0.5253 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1796/1796 [==============================] - 77s 43ms/step - loss: 2.2064 - accuracy: 0.6011 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1796/1796 [==============================] - 75s 42ms/step - loss: 1.8788 - accuracy: 0.6526 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 1.6466 - accuracy: 0.6891 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.4738 - accuracy: 0.7202 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.3444 - accuracy: 0.7412 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.2430 - accuracy: 0.7586 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 1.1660 - accuracy: 0.7720 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 1.1042 - accuracy: 0.7826 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 1.0555 - accuracy: 0.7911 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 1.0136 - accuracy: 0.7968 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.9797 - accuracy: 0.8038 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.9486 - accuracy: 0.8091 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.9283 - accuracy: 0.8127 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.9123 - accuracy: 0.8148 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8914 - accuracy: 0.8197 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8821 - accuracy: 0.8184 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8661 - accuracy: 0.8225 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8569 - accuracy: 0.8234 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.8478 - accuracy: 0.8248 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8434 - accuracy: 0.8248 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.8336 - accuracy: 0.8262 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8308 - accuracy: 0.8264 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8314 - accuracy: 0.8264 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8174 - accuracy: 0.8288 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.8200 - accuracy: 0.8269 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.8217 - accuracy: 0.8261 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.8195 - accuracy: 0.8261\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.8195 - accuracy: 0.8261 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.6719 - accuracy: 0.8567 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.6279 - accuracy: 0.8675 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.6137 - accuracy: 0.8689 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "1796/1796 [==============================] - 77s 43ms/step - loss: 0.6077 - accuracy: 0.8713 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 0.6003 - accuracy: 0.8730 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "1796/1796 [==============================] - 71s 40ms/step - loss: 0.5926 - accuracy: 0.8756 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5874 - accuracy: 0.8769 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "1796/1796 [==============================] - 75s 42ms/step - loss: 0.5815 - accuracy: 0.8774 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5789 - accuracy: 0.8777 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5774 - accuracy: 0.8784 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5724 - accuracy: 0.8801 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.5708 - accuracy: 0.8796 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5706 - accuracy: 0.8811 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5640 - accuracy: 0.8823 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5660 - accuracy: 0.8805 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.5609 - accuracy: 0.8825 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.5597 - accuracy: 0.8827 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5633 - accuracy: 0.8806 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.5603 - accuracy: 0.8813 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.5550 - accuracy: 0.8823\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.5551 - accuracy: 0.8822 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.5081 - accuracy: 0.8915 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4948 - accuracy: 0.8941 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4921 - accuracy: 0.8942 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4888 - accuracy: 0.8947 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "1796/1796 [==============================] - 74s 41ms/step - loss: 0.4840 - accuracy: 0.8966 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4826 - accuracy: 0.8973 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4815 - accuracy: 0.8959 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4800 - accuracy: 0.8972 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.8953\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "1796/1796 [==============================] - 66s 37ms/step - loss: 0.4807 - accuracy: 0.8953 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "1796/1796 [==============================] - 66s 37ms/step - loss: 0.4575 - accuracy: 0.9004 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4522 - accuracy: 0.9015 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4504 - accuracy: 0.9022 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4494 - accuracy: 0.9013 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4479 - accuracy: 0.9027 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4475 - accuracy: 0.9024 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "1796/1796 [==============================] - 71s 39ms/step - loss: 0.4481 - accuracy: 0.9019 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.4464 - accuracy: 0.9028\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4463 - accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4324 - accuracy: 0.9057 - lr: 6.2500e-05\n",
      "Epoch 72/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4339 - accuracy: 0.9063 - lr: 6.2500e-05\n",
      "Epoch 73/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4335 - accuracy: 0.9057 - lr: 6.2500e-05\n",
      "Epoch 74/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4329 - accuracy: 0.9050 - lr: 6.2500e-05\n",
      "Epoch 75/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.9056\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4316 - accuracy: 0.9056 - lr: 6.2500e-05\n",
      "Epoch 76/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4259 - accuracy: 0.9069 - lr: 3.1250e-05\n",
      "Epoch 77/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4238 - accuracy: 0.9077 - lr: 3.1250e-05\n",
      "Epoch 78/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4241 - accuracy: 0.9078 - lr: 3.1250e-05\n",
      "Epoch 79/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4226 - accuracy: 0.9077 - lr: 3.1250e-05\n",
      "Epoch 80/100\n",
      "1796/1796 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.9077\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4229 - accuracy: 0.9077 - lr: 3.1250e-05\n",
      "Epoch 81/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4200 - accuracy: 0.9086 - lr: 1.5625e-05\n",
      "Epoch 82/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4204 - accuracy: 0.9083 - lr: 1.5625e-05\n",
      "Epoch 83/100\n",
      "1796/1796 [==============================] - 67s 38ms/step - loss: 0.4214 - accuracy: 0.9076 - lr: 1.5625e-05\n",
      "Epoch 84/100\n",
      "1796/1796 [==============================] - 67s 37ms/step - loss: 0.4196 - accuracy: 0.9088 - lr: 1.5625e-05\n",
      "Epoch 85/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4177 - accuracy: 0.9096 - lr: 1.5625e-05\n",
      "Epoch 86/100\n",
      "1796/1796 [==============================] - 70s 39ms/step - loss: 0.4189 - accuracy: 0.9089 - lr: 1.5625e-05\n",
      "Epoch 87/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4195 - accuracy: 0.9087 - lr: 1.5625e-05\n",
      "Epoch 88/100\n",
      "1795/1796 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.9088\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4191 - accuracy: 0.9088 - lr: 1.5625e-05\n",
      "Epoch 89/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4175 - accuracy: 0.9088 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4175 - accuracy: 0.9094 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4179 - accuracy: 0.9085 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "1796/1796 [==============================] - 69s 39ms/step - loss: 0.4169 - accuracy: 0.9095 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "1796/1796 [==============================] - 73s 41ms/step - loss: 0.4173 - accuracy: 0.9094 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4162 - accuracy: 0.9093 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4179 - accuracy: 0.9082 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "1796/1796 [==============================] - 68s 38ms/step - loss: 0.4165 - accuracy: 0.9096 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4175 - accuracy: 0.9087 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4170 - accuracy: 0.9087 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4162 - accuracy: 0.9090 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "1796/1796 [==============================] - 69s 38ms/step - loss: 0.4182 - accuracy: 0.9079 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "rnn_r = rnn_model.fit(train_X, train_y, epochs=100,callbacks=[learning_rate_reduction,es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring metrics for lyrical output based on readabilty and rhyme frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_readability(input_lines):\n",
    "  avg_readability = 0\n",
    "  for line in input_lines:\n",
    "    avg_readability += textstat.automated_readability_index(line)\n",
    "  return avg_readability / len(input_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rhyme_density(lines):\n",
    "  total_syllables = 0\n",
    "  rhymed_syllables = 0\n",
    "  for line in lines:\n",
    "    for word in line.split():\n",
    "      p = pronouncing.phones_for_word(word)\n",
    "      if len(p) == 0:\n",
    "        break\n",
    "      syllables = pronouncing.syllable_count(p[0])\n",
    "      total_syllables += syllables\n",
    "      has_rhyme = False\n",
    "      for rhyme in pronouncing.rhymes(word):\n",
    "        if has_rhyme:\n",
    "          break\n",
    "        for idx, r_line in enumerate(lines):\n",
    "          if idx > 4:\n",
    "            break\n",
    "          if rhyme in r_line:\n",
    "            rhymed_syllables += syllables\n",
    "            has_rhyme = True\n",
    "            break\n",
    "  return rhymed_syllables/total_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_line(input_line, artists_lines, artists_avg_readability, artists_avg_rhyme_idx):\n",
    "  gen_readability = textstat.automated_readability_index(input_line)\n",
    "  gen_rhyme_idx = calc_rhyme_density(input_line)\n",
    "  comp_lines = compare_lines(input_line, artists_lines)\n",
    "\n",
    "  # Scores based off readability, rhyme index, and originality. The lower the score the better.\n",
    "  line_score = (artists_avg_readability - gen_readability) + (artists_avg_rhyme_idx - gen_rhyme_idx) + comp_lines\n",
    "  return line_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lines(input_line, artists_lines):\n",
    "  '''\n",
    "    input_lines are the fire lines our AI generates\n",
    "    artists_lines are the original lines for the artist\n",
    "\n",
    "    The lower the score the better! We want unique lines\n",
    "  '''\n",
    "  # Converts sentences to matrix of token counts\n",
    "  avg_dist = 0\n",
    "  total_counted = 0\n",
    "  for line in artists_lines:\n",
    "    v = CountVectorizer()\n",
    "    # Vectorize the sentences\n",
    "    word_vector = v.fit_transform([input_line, line])\n",
    "    # Compute the cosine distance between the sentence vectors\n",
    "    cos_dist = 1-pdist(word_vector.toarray(), 'cosine')[0]\n",
    "    if not math.isnan(cos_dist):\n",
    "      avg_dist += 1-pdist(word_vector.toarray(), 'cosine')[0]\n",
    "      total_counted += 1\n",
    "  return avg_dist/total_counted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model\n",
    "Model is run based on seed sentence. Seed sentence combines final words from previous line and markov generated sentence. Generates lines with syllable count close to required by a haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_line(seed_phrase, model, length_of_line):\n",
    "  seed_words = ' '.join(seed_phrase.split(' ')[-2:])\n",
    "  syl = 0 + syllables.estimate(seed_words)\n",
    "  while syl < length_of_line:\n",
    "    seed_tokens = pad_sequences(tokenizer.texts_to_sequences([seed_phrase]), maxlen=29)\n",
    "    output_p = model.predict(seed_tokens)\n",
    "    output_word = np.argmax(output_p, axis=1)[0]-1\n",
    "    syl += syllables.estimate(str(list(tokenizer.word_index.items())[output_word][0]))\n",
    "    seed_phrase += \" \" + str(list(tokenizer.word_index.items())[output_word][0])\n",
    "  return seed_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Haiku\n",
    "Generate haiku by using a user defined first line. Generate the next two lines by creating a defined number of attempts and selecting the highest scoring option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_haiku( model, intro_line, artists_lines, length_of_line, length_of_song=20, min_score_threshold=-0.2, max_score_threshold=0.2, tries=5):\n",
    "  artists_avg_readability = calc_readability(artists_lines)\n",
    "  artists_avg_rhyme_idx = calc_rhyme_density(artists_lines)\n",
    "  fire_song = [intro_line + \" \"]\n",
    "  line_lengths = [7, 5]\n",
    "  cur_tries = 0\n",
    "  candidate_lines = []\n",
    "\n",
    "  while len(fire_song) < 3:\n",
    "    try:\n",
    "        seed_sentence = markov_model.make_sentence(tries=100).split(\" \")\n",
    "        print('Seed Sentence: ', seed_sentence)\n",
    "        seed_sentence = \" \".join(fire_song[-1].split(' ')[-3:]) + \" \".join(seed_sentence[:2])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    line = generate_line(seed_sentence, model, line_lengths[len(fire_song)-1])\n",
    "    print(syllables.estimate(' '.join(line.split(' ')[2:])))\n",
    "    if (syllables.estimate(' '.join(line.split(' ')[2:])) == line_lengths[len(fire_song) - 1]):\n",
    "      cur_tries += 1\n",
    "      print(cur_tries)\n",
    "    #print(line)\n",
    "    line_score = score_line(line, lyrics, artists_avg_readability, artists_avg_rhyme_idx) \n",
    "    candidate_lines.append((line_score, line))\n",
    "\n",
    "\n",
    "    if line_score <= max_score_threshold and line_score >= min_score_threshold and (syllables.estimate(' '.join(line.split(' ')[3:])) == line_lengths[len(fire_song) - 1]):\n",
    "      fire_song.append(' '.join(line.split(' ')[2:]) + \" \")\n",
    "      cur_tries = 0\n",
    "      print(\"Generated line: \", len(fire_song))\n",
    "\n",
    "    if cur_tries >= tries:\n",
    "      lowest_score = np.Infinity\n",
    "      best_line = \"\"\n",
    "      for line in candidate_lines:\n",
    "        if (line[0] < lowest_score) and (syllables.estimate(' '.join(line[1].split(' ')[2:])) == line_lengths[len(fire_song) - 1]):\n",
    "          best_line = line[1]\n",
    "          candidate_lines = []\n",
    "      \n",
    "      fire_song.append(' '.join(best_line.split(' ')[2:]) + \" \")\n",
    "      print(\"Generated line: \", len(fire_song))\n",
    "      cur_tries = 0\n",
    "      \n",
    "  print(\"Generated song with avg rhyme density: \", calc_rhyme_density(fire_song), \"and avg readability of: \", calc_readability(fire_song))\n",
    "  return fire_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Sentence:  ['No', 'constellations', 'in', 'the', 'sky,', \"there's\", 'magic', 'in', 'the', 'air', 'and', 'echo']\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "7\n",
      "1\n",
      "Seed Sentence:  ['Like', 'a', 'child', 'I', 'fear', 'the', 'end,', 'but', 'for', 'what', 'reason?']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "7\n",
      "2\n",
      "Seed Sentence:  ['Or', 'do', 'they', 'just', 'think', \"I'm\", 'a', 'piece', 'of', 'me', 'Or', 'lead', 'me', 'to', 'the', 'ages,', 'the', 'darkness', 'is', 'calling']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "8\n",
      "Seed Sentence:  ['Their', 'future', 'in', 'the', 'red']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "10\n",
      "Seed Sentence:  ['I', 'will', 'face', 'my', 'afflictionsAnd', 'I', 'see', 'stars,', 'I', 'hear', 'the', 'voices', 'of', 'steel', 'in', 'the', 'night.', 'I', 'am', 'He.', 'He', 'the', 'carnal.', 'He', 'the', 'ravage.', 'He', 'the', 'superior.']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "7\n",
      "3\n",
      "Seed Sentence:  ['I', \"can't\", 'even', 'see', 'me']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "8\n",
      "Seed Sentence:  ['Scattered', 'across', 'in', 'black', 'all', 'I', 'see', 'is', 'dead', 'words']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "9\n",
      "Seed Sentence:  ['But', 'I', \"won't\", 'let', 'go']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "7\n",
      "4\n",
      "Seed Sentence:  ['And', 'I', 'shall', 'be', 'the', 'death', 'of', 'me', 'keeps', 'me', 'alive']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "8\n",
      "Seed Sentence:  ['Expose', 'my', 'cage,', 'and', 'I', 'will', 'never', 'win.']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "7\n",
      "5\n",
      "Seed Sentence:  ['Coming', 'from', 'each', 'side', 'to', 'keep', 'them', 'docile']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "7\n",
      "6\n",
      "Seed Sentence:  ['Not', 'stopping', 'till', \"you're\", 'fucking', 'dead', '(Iron', 'Balls!)']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "8\n",
      "Seed Sentence:  ['Ornamentation', 'woven', 'with', 'their', 'faces.', '(They', 'are', 'in', 'the', 'presence', 'of', 'a', 'sadly', 'absent', 'glory']\n",
      "7\n",
      "7\n",
      "Seed Sentence:  ['Our', 'enemies', 'in', 'the', 'eyes']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "7\n",
      "8\n",
      "Seed Sentence:  ['When', 'I', 'close', 'my', 'eyes,', 'and', 'give', 'you', 'to', 'the', 'bone', 'Although', \"she's\", 'gone,', 'my', 'love', 'will', 'never', \"dieI'll\", 'ever', 'have', 'to', 'paradise']\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "7\n",
      "9\n",
      "Seed Sentence:  ['Heavy', 'Metal,', 'rock', 'n', 'roll', 'we', 'praise']\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "7\n",
      "10\n",
      "Generated line:  2\n",
      "Seed Sentence:  ['this', 'is', 'not', 'my', 'home']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "7\n",
      "Seed Sentence:  ['On', 'the', 'day', 'I', 'die!']\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "6\n",
      "Generated line:  3\n",
      "Generated song with avg rhyme density:  0.25 and avg readability of:  2.8000000000000003\n"
     ]
    }
   ],
   "source": [
    "first_line = 'The earth is burning'\n",
    "rnn = generate_haiku( rnn_model, first_line, lyrics, length_of_line =12 , tries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song Generated with SimpleRNN:\n",
      "The earth is burning \n",
      "Heavy Metal, disaster \n",
      "On the you scream love \n"
     ]
    }
   ],
   "source": [
    "print(\"Song Generated with SimpleRNN:\")\n",
    "for line in rnn:\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the realm of creative experimentation, this project unites neural networks, the evocative world of metal lyrics, and the succinct beauty of haikus. However, it's important to recognize that the resulting haikus, while aesthetically appealing and steeped in the imagery of metal motifs, lack profound meaning or genuine depth.\n",
    "\n",
    "The allure of the haikus predominantly emerges from the masterful arrangement of words, drawing inspiration from the thematic elements of metal culture. The neural network's ability to replicate the structural integrity of haikus, combined with the raw intensity often found in metal lyrics, leads to the formation of verses that resonate with the senses.\n",
    "\n",
    "Yet, beneath the captivating surface, these haikus are a testament to the challenges of imbuing AI-generated art with true human-like understanding. While they mimic the form and language of both metal lyrics and traditional haikus, they ultimately reveal the current limitations of AI in grasping nuanced emotional contexts and conveying authentic artistic expression.\n",
    "\n",
    "Scraping code can be found [here](https://github.com/patrick-naylor/Metal_Lyric_Generator/blob/main/scraping.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('RE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36173ce3d0ba65c4d5858397b29ed022201e85e6fa210e17017ccab12f085240"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
